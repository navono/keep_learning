{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CNN\n",
    "\n",
    "\n",
    "CNN 通常用于计算机视觉任务。CNN 通过在输入数据上滑动卷积核来提取特征。卷积核是一个小的矩阵，它在输入数据上滑动，计算输入数据和卷积核之间的点积。卷积核的大小通常是 3x3 或 5x5。卷积核的大小和数量是超参数，需要根据具体任务进行调整。CNN 的变体有： ResNets、DenseNets 和 Octave 卷积。"
   ],
   "id": "6b2a8639b91cf626"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Neural Networks and Representation Learning\n",
    "\n",
    "在之前的章节中，展示了两个例子，第一个是房屋预测，第二个是 MNIST 预测。在这两个例子中，带隐藏层的神经网络效果均要好于不带隐藏层的神经网络。原因是隐藏层提取了输入数据和输出数据之间的非线性的特征。\n",
    "\n",
    "然而，一个更普遍的原因是，在机器学习中，我们经常需要原始特征的线性组合，以便有效地预测我们的目标。例如在 MNIST 中，单张图片有 784 个像素，不同像素的组合可以表示不同的数字，所有这些组合都会对图像是某一特定数字的概率产生积极或消极的影响。神经网络可以通过训练过程自动发现重要的原始特征组合。这一过程首先是通过与随机权重矩阵相乘来创建原始特征的初始随机组合；通过训练，神经网络学会提炼出有用的组合，并摒弃那些没用的组合。这种学习哪些特征组合是重要的过程被称为 `表征学习`，也是神经网络在不同领域取得成功的主要原因。\n",
    "\n",
    "![CNN](./images/05_cnn1.png)\n",
    "\n",
    "\n",
    "### A Different Architecture for Image Data\n",
    "\n",
    "\n",
    "同样也是创建特征的组合，但是是在更高层次并且是更高的数量级上。每个特征的组合只是输入图像中一小块矩形区域的像素组合。\n",
    "\n",
    "![CNN](./images/05_cnn2.png)\n",
    "\n",
    "让神经网络学习所有特征的组合，也就是所有像素的组合是困难且低效的，且忽略了前面的一个见解（Insight）：图像中大部分重要的特征组合都出现在小矩形图像中。从这些小矩形中抽取特征并组合的操作是卷积操作。卷积操作是一种特殊的矩阵乘法，它在输入数据上滑动一个小的矩阵（卷积核），计算输入数据和卷积核之间的点积。卷积核的大小通常是 3x3 或 5x5。卷积核的大小和数量是超参数，需要根据具体任务进行调整。\n"
   ],
   "id": "fd179c53480f3257"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The Convolution Operation\n",
    "\n",
    "\n",
    "在设定卷积核大小后，卷积核应用在图像上的不同位置。例如我们有以下输入图像的数据：\n",
    "$$\n",
    "I = \\begin{bmatrix}\n",
    "i_{1,1} & i_{1,2} & i_{1,3} & i_{1,4} & i_{1,5} \\\\\n",
    "i_{2,1} & i_{2,2} & i_{2,3} & i_{2,4} & i_{2,5} \\\\\n",
    "i_{3,1} & i_{3,2} & i_{3,3} & i_{3,4} & i_{3,5} \\\\\n",
    "i_{4,1} & i_{4,2} & i_{4,3} & i_{4,4} & i_{4,5} \\\\\n",
    "i_{5,1} & i_{5,2} & i_{5,3} & i_{5,4} & i_{5,5} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "假设我们有一个 3x3 的卷积核：\n",
    "$$\n",
    "K = \\begin{bmatrix}\n",
    "k_{1,1} & k_{1,2} & k_{1,3} \\\\\n",
    "k_{2,1} & k_{2,2} & k_{2,3} \\\\\n",
    "k_{3,1} & k_{3,2} & k_{3,3} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "假设我们将卷积核作用在 $I$ 中间的 (3,3) 位置，结果记为 $o_33$ （o 表示 output）\n",
    "$$\n",
    "o_{33} = w_{1,1} \\times i_{2,2} + w_{1,2} \\times i_{2,3} + w_{1,3} \\times i_{2,4} + w_{2,1} \\times i_{3,2} + w_{2,2} \\times i_{3,3} + w_{2,3} \\times i_{3,4} + w_{3,1} \\times i_{4,2} + w_{3,2} \\times i_{4,3} + w_{3,3} \\times i_{4,4}\n",
    "$$\n",
    "\n",
    "这个值会像我们在神经网络中看到的其他计算特征一样被处理：它可能会被添加一个偏置，然后可能会被送入一个激活函数，然后它将代表一个 \"神经元 \"或 \"学习特征\"，并传递给网络的后续层\n",
    "\n",
    "根据不同卷积核的类型，也称为\"模式提取\"（pattern detectors），可以提取图像中不同属性的特征。例如下面的 3x3 卷积核可以检测图像中的边缘：\n",
    "$$\n",
    "K = \\begin{bmatrix}\n",
    "0& 1 & 0 \\\\\n",
    "1 & 4 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "已知还有类似的矩阵可以检测是否存在角、是否存在垂直线或水平线，等等。这些卷积核的选择是超参数，需要根据具体任务进行调整。\n",
    "\n",
    "现在假设我们使用同一组权重W来检测由W定义的视觉模式是否存在于输入图像中的每个位置。我们可以想象“在输入图像上滑动 W”，取 W 与图像每个位置处的像素的点积，最终得到一个与原始图像大小几乎相同的新图像 O（可能略有不同，具体取决于关于我们如何处理边缘）。图像 O 将是一种 \"特征图\"，显示输入图像中出现 W 所定义图案的位置。这种操作实际上就是卷积神经网络中的操作；它被称为卷积，其输出实际上被称为特征图。\n",
    "\n",
    "在进入 `Operation` 前，我们给卷积再增加点维度。"
   ],
   "id": "159f4be31a2d4d30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multichannel Convolution Operation\n",
    "\n",
    "回顾一下：卷积神经网络与普通神经网络的不同之处在于，它们创建的特征要多出一个数量级，而且每个特征都是输入图像中一小块区域的函数。现在我们可以说得更具体一些：从 n 个输入像素开始，刚才描述的卷积操作将创建 n 个输出特征，输入图像中的每个位置（区域）都有一个特征。\n",
    "\n",
    "在神经网络的卷积层中实际发生的情况则更进一步：在这里，我们将创建 n 个特征的 f 个集合，每个集合都有相应的（初始随机的）权重集，这些权重集定义了一种视觉模式，输入图像中每个位置的检测结果都将被捕捉到特征图中。这 f 个特征图将通过 f 次卷积操作创建。这些特征图将被堆叠在一起，形成一个新的多通道特征图，这个特征图将被传递到网络的下一层。\n",
    "\n",
    "![CNN](./images/05_cnn3.png)\n",
    "\n",
    "通过特定权重集检测到的每个 \"特征集 \"被称为一个特征图，而在卷积层中，特征图的数量被称为卷积层的通道数--这就是为什么卷积层所涉及的操作被称为多通道卷积。此外，f 组权重 W 被称为卷积滤波器（convolutional filters）。"
   ],
   "id": "95526f443f0fff1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Convolutional Layers\n",
    "\n",
    "在我们之前的模型中，`Layer` 接收二维的 `ndarray` 作为输入，输出一个二维的 `ndarray`。在卷积神经网络中，`Layer` 接收三维的 `ndarray` 作为输入，输出一个三维的 `ndarray`。这个三维的 `ndarray` 有两个维度是空间维度（高、宽），一个维度是通道维度。这里就引发了另外一个问题：我们要如何将这个 `ndarray` 传递给下一层的卷积网络从而创建 “深度卷积” 神经网络？\n",
    "\n",
    "如果卷积层的输出是一个 m 个通道 × 图像高度 × 图像宽度的三维数组，那么图像中 m 个特征图中的一个给定位置就是对上一层中每个相应特征图中的该位置进行不同滤波器卷积的线性组合。这样，m 个滤波器图中的每个位置就代表了先前卷积层已学习到的 m 个视觉特征的组合。"
   ],
   "id": "cbbbf4bc7cb94921"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 多通道卷积的操作\n",
    "\n",
    "#### 输入参数的维度\n",
    "- Batch Size\n",
    "- Input channels\n",
    "- Image height\n",
    "- Image width\n",
    "\n",
    "#### 输出参数的维度\n",
    "- Batch Size\n",
    "- Output channels\n",
    "- Image height\n",
    "- Image width\n",
    "\n",
    "### 卷积核的维度\n",
    "- Input channels\n",
    "- Output channels\n",
    "- Kernel height\n",
    "- Kernel width"
   ],
   "id": "d6eafce0e1bc5596"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 卷积层与全连接层的区别\n",
    "\n",
    "![CNN](./images/05_cnn4.png)\n",
    "\n",
    "\n",
    "此外，这两种神经层的最后一个区别在于对单个神经元本身的解释方式：\n",
    "\n",
    "- 对全连接层中神经元的解释是，它检测当前观测结果中是否存在前一层所学特征的特定组合。\n",
    "- 对卷积层中神经元的解释是，它检测输入图像的给定位置是否存在前一层学习到的特定视觉模式组合。\n",
    "\n",
    "\n",
    "将卷基层纳入神经网络之前，还需要解决一个问题：如何利用我们获得的输出维度的 `ndarray` 进行预测？"
   ],
   "id": "464359bf2752deec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 使用卷积层进行预测：Flatten Layer\n",
    "\n",
    "在上一章中使用全连接神经网络预测图像属于 10 个类别中的哪一个时，我们只需确保最后一层的维度为 10；然后我们就可以将这 10 个数字输入 softmax cross entropy 函数，以确保它们被解释为概率。\n",
    "\n",
    "在卷积层中，我们有一个三维 `ndarray`，每个观测点的形状为 m 个通道 × 图像高度 × 图像宽度，每个神经元只是表示在图像的特定位置是否存在视觉特征的特定组合（如果是深度卷积神经网络，则可能是特征的特征或特征的特征）。这与我们将一个全连接神经网络应用于该图像所学习到的特征并无不同：第一个全连接层代表单个像素的特征，第二个全连接层代表这些特征的特征，以此类推。而在全连接架构中，我们只需将网络学习到的每个 \"特征的特征\" 视为一个神经元，将其作为预测图像所属类别的输入。\n",
    "\n",
    "在卷积神经网络中也是一样，我们将 `m` 个特征图视为 m 个通道 × 图像高度 × 图像宽度个神经元，使用 `Flatten` 操作将其转换为一个一维 `ndarray`，然后将其输入到 softmax cross entropy 函数中，以确保它们被解释为概率。\n",
    "\n",
    "在进入实现 `Flatten` 操作前，我们再看个在卷积神经网络中常用的操作：`Pooling`。"
   ],
   "id": "6bbbaa67573d5660"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Pooling Layers\n",
    "\n",
    "池化层是卷积神经网络中常用的另一种层。它们对卷积操作创建的每个特征图进行简单的降采样；最常用的池化大小为 2，这涉及将每个特征图的每个 2 × 2 部分映射为该部分的最大值（在最大池化的情况下）或该部分的平均值（在平均池化的情况下）。\n",
    "\n",
    "![Pooling](./images/05_pooling.png)\n",
    "\n",
    "\n",
    "池化层的主要有点在于降低了计算量。但也有很多人对池化层的必要性提出了置疑。现如今很多的 CNN 已经很少或者没有了池化层。\n"
   ],
   "id": "6271d131e10c139a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 将 CNN 应用于图像之外\n",
    "\n",
    "到目前为止，我们所描述的一切，都是使用神经网络处理图像的极其标准的方法：图像通常表示为一组 m 个像素通道，其中 m = 1 表示黑白图像，m = 3 表示彩色图像，然后对每个通道进行一定数量的 m 次卷积运算（使用前面解释过的 m1×m2 滤波图），这种模式会持续几个层。这些知识点在其他的资料中也会介绍，但是它们没介绍的是，CNN 也会被利用在图像之外的任务中，比如 DeepMind 的 AlphaGo 就是使用 CNN 来处理围棋的棋盘状态。\n",
    "\n",
    "CNN 及其多通道卷积操作通常应用于图像，但用多个 \"通道 \"来表示沿某个空间维度排列的数据这一更为普遍的想法甚至适用于图像之外的其他领域。"
   ],
   "id": "66b236824e6a9d3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 实现多通道卷积操作\n",
    "\n",
    "- 执行卷积操作\n",
    "- 填充"
   ],
   "id": "a9c32f713dcbb518"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray"
   ],
   "id": "1768fbfda12c3cc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def assert_same_shape(output: ndarray,\n",
    "                      output_grad: ndarray):\n",
    "    assert output.shape == output_grad.shape, \\\n",
    "        '''\n",
    "        Two ndarray should have the same shape; instead, first ndarray's shape is {0}\n",
    "        and second ndarray's shape is {1}.\n",
    "        '''.format(tuple(output_grad.shape), tuple(output.shape))\n",
    "    return None\n",
    "\n",
    "\n",
    "def assert_dim(t: ndarray,\n",
    "               dim: ndarray):\n",
    "    assert len(t.shape) == dim, \\\n",
    "        '''\n",
    "        Tensor expected to have dimension {0}, instead has dimension {1}\n",
    "        '''.format(dim, len(t.shape))\n",
    "    return None"
   ],
   "id": "92973f3747b7d484",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 向前传播\n",
    "\n",
    "1D 的卷积网络本质上是和 2D 的卷积网络是一样的。接收一个 1D 的输入和一个 1D 的卷积核，在输入上进行卷积操作，输出结果。\n",
    "\n",
    "假设我们的输入：\n",
    "$$\n",
    "Input:  \n",
    "\\begin{bmatrix}\n",
    "t_1, t_2, t_3, t_4, t_5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "卷积核：\n",
    "$$\n",
    "Kernel:\n",
    "\\begin{bmatrix}\n",
    "w_1, w_2, w_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "第一个元素的输出：\n",
    "$$\n",
    "Output Feature 1:\n",
    "t_1 w_1 + t_2 w_2 + t_3 w_3\n",
    "$$\n",
    "\n",
    "第二个元素的输出是在第一个元素的基础上，卷积核往右移动一个单位：\n",
    "$$\n",
    "Output Feature 2:\n",
    "t_2 w_1 + t_3 w_2 + t_4 w_3\n",
    "$$\n",
    "\n",
    "以此类推。"
   ],
   "id": "6b5dfedc1815e14e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def conv_1d(input: ndarray, param: ndarray) -> ndarray:\n",
    "    # assert correct dimensions\n",
    "    assert_dim(input, 1)\n",
    "    assert_dim(param, 1)\n",
    "\n",
    "    # pad the input\n",
    "    param_len = param.shape[0]\n",
    "    param_mid = param_len // 2\n",
    "    input_padded = _pad_1d(input, param_mid)\n",
    "\n",
    "    # initialize the output\n",
    "    output = np.zeros(input.shape)\n",
    "\n",
    "    # perform the 1d convolution\n",
    "    for o in range(output.shape[0]):\n",
    "        for p in range(param_len):\n",
    "            output[o] += param[p] * input_padded[o + p]\n",
    "\n",
    "    assert_same_shape(output, input)\n",
    "    return output"
   ],
   "id": "1159e235140e0ed4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Padding\n",
    "\n",
    "\n",
    "目的是为了输出的大小与输入保持一致。"
   ],
   "id": "d470aef625b2e844"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1 input， 1 output\n",
    "\n",
    "input_1d = np.array([1, 2, 3, 4, 5])\n",
    "kernel_1d = np.array([1, 1, 1])"
   ],
   "id": "d78ba064373e6a8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _pad_1d(input: ndarray,\n",
    "            padding: int) -> ndarray:\n",
    "    zero = np.array([0])\n",
    "    zero = np.repeat(zero, padding)\n",
    "    return np.concatenate([zero, input, zero])"
   ],
   "id": "967b7d7a127fe5e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "_pad_1d(input_1d, 1)",
   "id": "3de0b516e815cd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def conv_1d_sum(input: ndarray, param: ndarray) -> ndarray:\n",
    "    return np.sum(conv_1d(input, param))"
   ],
   "id": "4eea32d69fc995f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "conv_1d_sum(input_1d, kernel_1d)",
   "id": "a53192e0bdc82e07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Stride\n",
    "\n",
    "在介绍反向传播前，我们先来看看卷积操作的另一个超参数：Stride。Stride 是卷积核在输入数据上滑动的步长。在之前的例子中，Stride 是 1，也就是卷积核每次移动一个位置。如果 Stride 是 2，那么卷积核每次移动两个位置。Stride 是一个超参数，需要根据具体任务进行调整。\n",
    "\n",
    "步长的大小也会影响输出的大小。假设输入数据的大小是 $n$，卷积核的大小是 $f$，填充的大小是 $p$，步长是 $s$，那么输出数据的大小是：\n",
    "$$\n",
    "\\frac{n - f + 2p}{s} + 1\n",
    "$$"
   ],
   "id": "1d888c594078fac0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 卷积：向后传播\n",
    "\n",
    "我们需要计算\n",
    "- 损失相对于卷积运算输入的每个元素的偏导数\n",
    "- 损失相对于卷积核的偏导数\n",
    "\n",
    "\n",
    "在之前的 `ParamOperation` 的 `backward` 方法中，接收一个输出的梯度（表示的是输出的每一个元素最终对 Loss 的影像度），然后使用这个输出梯度计算输入和参数的梯度。也就是接收 `output_grad`，然后计算 `input_grad` 和 `param_grad`，`output_grade` 的大小和 `input_grad`、`param_grad` 大小是一样的。"
   ],
   "id": "9a87c1db4ce69800"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(190220)\n",
    "print(np.random.randint(0, input_1d.shape[0]))\n",
    "print(np.random.randint(0, kernel_1d.shape[0]))"
   ],
   "id": "a4348bec227e42cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_1d_2 = np.array([1, 2, 3, 4, 6])\n",
    "param_1d = np.array([1, 1, 1])"
   ],
   "id": "fdcd5495ddfa9f24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(conv_1d_sum(input_1d_2, param_1d) - conv_1d_sum(input_1d_2, param_1d))",
   "id": "eef8ee5eb865b065",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_1d = np.array([1, 2, 3, 4, 5])\n",
    "param_1d_2 = np.array([2, 1, 1])\n",
    "\n",
    "print(conv_1d_sum(input_1d, param_1d_2) - conv_1d_sum(input_1d, param_1d))\n",
    "\n",
    "print(conv_1d_sum(input_1d, param_1d))\n",
    "print(conv_1d_sum(input_1d_2, param_1d))\n",
    "\n",
    "# input_1d_2 的第5个元素值是 6。计算出的梯度是：41 - 39 = 2。下面解释为什么是 2。"
   ],
   "id": "22e876cbddbf6d27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### 计算 1D 卷积网络的输入梯度\n",
    "\n",
    "先看下 `input_1d` 的梯度，由 `_pad_1d` 可得 `input_1d` 进行了一个填充，因此长度是 $t_0 \\cdots t_6$。按照前面的向前传播的规则，我们可以得到：\n",
    "\n",
    "\n",
    "$$\n",
    "Output: \n",
    "\\begin{bmatrix}\n",
    "t_0 w_1 + t_1 w_2 + t_2 w_3 \\\\\n",
    "t_1 w_1 + t_2 w_2 + t_3 w_3 \\\\\n",
    "t_2 w_1 + t_3 w_2 + t_4 w_3 \\\\\n",
    "t_3 w_1 + t_4 w_2 + t_5 w_3 \\\\\n",
    "t_4 w_1 + t_5 w_2 + t_6 w_3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ],
   "id": "2cac2e6ec5fd95dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "特征输出：$O_1$ 是矩阵的第一行，$O_2$ 是矩阵的第二行，以此类推。我们在上述的 `input_1d_2` 中，将 $t_5$ 的值进行了修改，而 $t_5$ 出现在了\n",
    "- $O_4$\n",
    "- $O_5$\n",
    "\n",
    "因为卷积核的长度是 3，如果还有 $O_6$ 的话，那么 $t_5$ 也会出现在 $O_6$ 中。最终 $t_5$ 对损失的影响，记 $\\frac{\\partial L}{\\partial t_5} $ 为：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial t_5} = \\frac{\\partial L}{\\partial O_4} \\times w_3 + \\frac{\\partial L}{\\partial O_5} \\times w_2 + \\frac{\\partial L}{\\partial O_6} \\times w_1\n",
    "$$\n",
    "\n",
    "在上面的例子中，$\\frac{\\partial L}{\\partial O_i} = 1$，因此 $w_2 + w_3 = 2$。\n",
    "\n",
    "\n",
    "现在描述一般情况。将第 $i$ 个元素的输出梯度记为 $O^{grad}_{i}$（最终通过访问 `output_grad[i] 获取值`），那么：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial t_5} =  O^{grad}_{4} \\times w_3 + O^{grad}_{5} \\times w_2 + O^{grad}_{6} \\times w_1\n",
    "$$\n",
    "\n",
    "同理可得 $t_4$ 的梯度：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial t_4} =  O^{grad}_{3} \\times w_3 + O^{grad}_{4} \\times w_2 + O^{grad}_{5} \\times w_1\n",
    "$$"
   ],
   "id": "ab00a76b96901b19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T00:43:28.507462Z",
     "start_time": "2024-06-27T00:43:28.499221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# param: in our case a ndarray of shape (1,3)\n",
    "# param_len: the integer 3\n",
    "# input: in our case a ndarray of shape (1,5)\n",
    "# input_grad: always an ndarray the same shape as \"input\"\n",
    "# output_pad: in our case a ndarray of shape (1,7)\n",
    "for o in range(input.shape[0]):\n",
    "    for p in range(param.shape[0]):\n",
    "        input_grad[o] += output_pad[o + param_len - p - 1] * param[p]"
   ],
   "id": "86476c2d234852e2",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[35], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m o \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]):\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(param\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]):\n\u001B[1;32m----> 8\u001B[0m         input_grad[o] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43moutput_pad\u001B[49m\u001B[43m[\u001B[49m\u001B[43mo\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mparam_len\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m*\u001B[39m param[p]\n",
      "\u001B[1;31mIndexError\u001B[0m: index 5 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T01:15:30.111266Z",
     "start_time": "2024-06-27T01:15:30.108174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _input_grad_1d(input: ndarray,\n",
    "                   param: ndarray,\n",
    "                   output_grad: ndarray = None) -> ndarray:\n",
    "    param_len = param.shape[0]\n",
    "    param_mid = param_len // 2\n",
    "    # inp_pad = _pad_1d(input, param_mid)\n",
    "\n",
    "    if output_grad is None:\n",
    "        # 因为 1D 中的卷积操作是线性的，所以输出的梯度是 1\n",
    "        output_grad = np.ones_like(input)\n",
    "    else:\n",
    "        assert_same_shape(input, output_grad)\n",
    "\n",
    "    output_pad = _pad_1d(output_grad, param_mid)\n",
    "\n",
    "    # Zero padded 1 dimensional convolution\n",
    "    param_grad = np.zeros_like(param)\n",
    "    input_grad = np.zeros_like(input)\n",
    "\n",
    "    print('output_pad: ', output_pad)\n",
    "    print('input_grad', input_grad)\n",
    "    print('\\n')\n",
    "\n",
    "    for o in range(input.shape[0]):\n",
    "        print('input idx: ', o)\n",
    "        for f in range(param.shape[0]):\n",
    "            idx = o + param_len - f - 1\n",
    "            input_grad[o] += output_pad[idx] * param[f]\n",
    "            print(f'output_pad[{idx}]: ', output_pad[o + param_len - f - 1])\n",
    "            print(f'input_grad: ', input_grad)\n",
    "        print('\\n')\n",
    "    assert_same_shape(param_grad, param)\n",
    "\n",
    "    return input_grad"
   ],
   "id": "5d6e14a5143bf83a",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T01:15:30.461780Z",
     "start_time": "2024-06-27T01:15:30.456779Z"
    }
   },
   "cell_type": "code",
   "source": "_input_grad_1d(input_1d, param_1d)",
   "id": "ad2c1c2ee619c9f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_pad:  [0 1 1 1 1 1 0]\n",
      "input_grad [0 0 0 0 0]\n",
      "\n",
      "\n",
      "input idx:  0\n",
      "output_pad[2]:  1\n",
      "input_grad:  [1 0 0 0 0]\n",
      "output_pad[1]:  1\n",
      "input_grad:  [2 0 0 0 0]\n",
      "output_pad[0]:  0\n",
      "input_grad:  [2 0 0 0 0]\n",
      "\n",
      "\n",
      "input idx:  1\n",
      "output_pad[3]:  1\n",
      "input_grad:  [2 1 0 0 0]\n",
      "output_pad[2]:  1\n",
      "input_grad:  [2 2 0 0 0]\n",
      "output_pad[1]:  1\n",
      "input_grad:  [2 3 0 0 0]\n",
      "\n",
      "\n",
      "input idx:  2\n",
      "output_pad[4]:  1\n",
      "input_grad:  [2 3 1 0 0]\n",
      "output_pad[3]:  1\n",
      "input_grad:  [2 3 2 0 0]\n",
      "output_pad[2]:  1\n",
      "input_grad:  [2 3 3 0 0]\n",
      "\n",
      "\n",
      "input idx:  3\n",
      "output_pad[5]:  1\n",
      "input_grad:  [2 3 3 1 0]\n",
      "output_pad[4]:  1\n",
      "input_grad:  [2 3 3 2 0]\n",
      "output_pad[3]:  1\n",
      "input_grad:  [2 3 3 3 0]\n",
      "\n",
      "\n",
      "input idx:  4\n",
      "output_pad[6]:  0\n",
      "input_grad:  [2 3 3 3 0]\n",
      "output_pad[5]:  1\n",
      "input_grad:  [2 3 3 3 1]\n",
      "output_pad[4]:  1\n",
      "input_grad:  [2 3 3 3 2]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, 3, 2])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### 计算 1D 卷积网络的参数梯度\n",
    "\n",
    "\n",
    "计算参数的梯度与计算输入的梯度类似。\n",
    "\n",
    "$$\n",
    "w^{grad}_{1} = t_0 \\times O^{grad}_{1} + t_1 \\times O^{grad}_{2} + t_2 \\times O^{grad}_{3} + t_3 \\times O^{grad}_{4} + t_4 \\times O^{grad}_{5}\n",
    "$$"
   ],
   "id": "3bad94a75700b1ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T00:58:32.578177Z",
     "start_time": "2024-06-27T00:58:32.575757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(conv_1d_sum(input_1d, param_1d))\n",
    "print(conv_1d_sum(input_1d, param_1d_2))"
   ],
   "id": "d28edc54f90e49d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.0\n",
      "49.0\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# param: in our case a ndarray of shape (1,3)\n",
    "# param_len: the integer 3\n",
    "# input: in our case a ndarray of shape (1,5)\n",
    "# input_grad: always an ndarray the same shape as \"input\"\n",
    "# output_pad: in our case a ndarray of shape (1,7)\n",
    "for o in range(input.shape[0]):\n",
    "    for p in range(param.shape[0]):\n",
    "        param_grad[o] += input_pad[o + p] * output_grad[o]"
   ],
   "id": "fa5a5c6a79f93395"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T01:11:24.301228Z",
     "start_time": "2024-06-27T01:11:24.297871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _param_grad_1d(input: ndarray,\n",
    "                   param: ndarray,\n",
    "                   output_grad: ndarray = None) -> ndarray:\n",
    "    param_len = param.shape[0]\n",
    "    param_mid = param_len // 2\n",
    "    input_pad = _pad_1d(input, param_mid)\n",
    "\n",
    "    if output_grad is None:\n",
    "        output_grad = np.ones_like(input)\n",
    "    else:\n",
    "        assert_same_shape(inp, output_grad)\n",
    "\n",
    "    # Zero padded 1 dimensional convolution\n",
    "    param_grad = np.zeros_like(param)\n",
    "    input_grad = np.zeros_like(input)\n",
    "\n",
    "    print('input_pad: ', input_pad)\n",
    "    print('output_grad', output_grad)\n",
    "\n",
    "    for o in range(input.shape[0]):\n",
    "        print('input idx: ', o)\n",
    "        for p in range(param.shape[0]):\n",
    "            param_grad[p] += input_pad[o + p] * output_grad[o]\n",
    "            print('input_pad[o + p]: ', input_pad[o + p])\n",
    "            print(f'param_grad: ', param_grad)\n",
    "        print('\\n')\n",
    "\n",
    "    assert_same_shape(param_grad, param)\n",
    "\n",
    "    return param_grad"
   ],
   "id": "25bd9ae77158897f",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T01:11:24.571051Z",
     "start_time": "2024-06-27T01:11:24.566335Z"
    }
   },
   "cell_type": "code",
   "source": "_param_grad_1d(input_1d, param_1d)",
   "id": "6e60ae2c388cdde8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_pad:  [0 1 2 3 4 5 0]\n",
      "output_grad [1 1 1 1 1]\n",
      "input idx:  0\n",
      "input_pad[o + p]:  0\n",
      "param_grad:  [0 0 0]\n",
      "input_pad[o + p]:  1\n",
      "param_grad:  [0 1 0]\n",
      "input_pad[o + p]:  2\n",
      "param_grad:  [0 1 2]\n",
      "\n",
      "\n",
      "input idx:  1\n",
      "input_pad[o + p]:  1\n",
      "param_grad:  [1 1 2]\n",
      "input_pad[o + p]:  2\n",
      "param_grad:  [1 3 2]\n",
      "input_pad[o + p]:  3\n",
      "param_grad:  [1 3 5]\n",
      "\n",
      "\n",
      "input idx:  2\n",
      "input_pad[o + p]:  2\n",
      "param_grad:  [3 3 5]\n",
      "input_pad[o + p]:  3\n",
      "param_grad:  [3 6 5]\n",
      "input_pad[o + p]:  4\n",
      "param_grad:  [3 6 9]\n",
      "\n",
      "\n",
      "input idx:  3\n",
      "input_pad[o + p]:  3\n",
      "param_grad:  [6 6 9]\n",
      "input_pad[o + p]:  4\n",
      "param_grad:  [ 6 10  9]\n",
      "input_pad[o + p]:  5\n",
      "param_grad:  [ 6 10 14]\n",
      "\n",
      "\n",
      "input idx:  4\n",
      "input_pad[o + p]:  4\n",
      "param_grad:  [10 10 14]\n",
      "input_pad[o + p]:  5\n",
      "param_grad:  [10 15 14]\n",
      "input_pad[o + p]:  0\n",
      "param_grad:  [10 15 14]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([10, 15, 14])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Batches, 2D 卷积和多通道",
   "id": "8ddc60cd1d745b68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Convolutional Neural Networks",
   "id": "7580e67bb4eb3a1a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
