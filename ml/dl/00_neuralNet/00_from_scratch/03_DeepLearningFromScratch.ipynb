{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Deep Learning From Scratch\n",
    "\n",
    "\n",
    "如果我们将模型定义为一个函数，并将参数作为其某些运算的输入，那么我们就可以使用以下程序对其进行 \"拟合\"，从而以最佳方式描述数据：\n",
    "\n",
    "1、将观察数据反复传递给模型，在 \"前向传递 \"过程中跟踪沿途计算的计算量（quantities）\n",
    "2、计算损失，表示模型的预测与预期输出或目标的偏差程度\n",
    "3、利用前向传递的计算量和第 1 章中的链式法则计算出的结果，计算每个输入参数最终对损耗的影响程度\n",
    "4、更新参数值，以便下一组观测数据通过模型时，损失有望减少\n",
    "\n",
    "为了能够说明以及应对模型的复杂性和遍历性，需要将前两章的内容进行一个整合，以便能够更好地理解模型的工作原理。也就是需要创建一些高层级的对象。"
   ],
   "id": "9128238a01f61072"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 神经网络的构建块：Operations\n",
    "\n",
    "`Operation` 类将代表神经网络中的一个组成函数，它有前向（`forward`）和反向（`backward`）方法，每个方法都接受 `ndarray` 作为输入，并产生 `ndarray` 的输出。\n",
    "\n",
    "至少有两种类型的 `Operation` 类。第一类入矩阵乘法，第二类就是激活函数。\n",
    "\n",
    "在矩阵乘法中，输入为 `ndarray`，返回一个不同维度的 `ndarray`。在激活函数中，例如 `sigmoid`，是对类型为 `ndarray` 的输入的每个元素应用某个函数（操作）。\n",
    "\n",
    "`ndarray` 是如何通过 `Operation` 的呢？\n",
    "\n",
    "每个 `Operation` 中，在前向传递（`forward pass`）时，向前计算输出，在反向传递（`backward pass`）时，接收梯度（“输出的梯度”），也就是 `loss` 相对于每个输出元素的偏导数。\n",
    "在反向传递（`backward pass`）中，每个 `Operation` 都会向后发送梯度（“输入的梯度”），也就是 `loss` 相对于每个输入元素的偏导数。\n",
    "\n",
    "根据上述描述，实现 `Operation` 类有以下约束条件：\n",
    "- 输出梯度（output gradient）的 `ndarray` 的形状与输出（output）的 `ndarray` 的形状必须相同\n",
    "- 在反向传递过程中的向后发送的输入梯度（input gradient）的 `ndarray` 的形状与输入（input）的 `ndarray` 的形状必须相同\n",
    "\n",
    "\n",
    "![Operation](./images/03_an_operation.png)\n",
    "\n",
    "\n",
    "\n",
    "带权重\n",
    "\n",
    "![Operation With Weight](./images/03_an_operation_with_param.png)\n",
    "\n",
    "\n",
    "\n",
    "对输入（input）进行前向传播（forward pass）计算输出（output），在反响传播中计算输入梯度（input gradient）。前向传播中的输入的形状与反向传播中的输入的形状是相同的。"
   ],
   "id": "606a241e265c541b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:06.135055Z",
     "start_time": "2024-06-07T00:36:06.051614Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "from typing import List"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:08.415605Z",
     "start_time": "2024-06-07T00:36:08.412822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def assert_same_shape(array: ndarray,\n",
    "                      array_grad: ndarray):\n",
    "    assert array.shape == array_grad.shape, \\\n",
    "        '''\n",
    "        Two ndarray's should have the same shape;\n",
    "        instead, first ndarray's shape is {0}\n",
    "        and second ndarray's shape is {1}.\n",
    "        '''.format(tuple(array_grad.shape), tuple(array.shape))\n",
    "    return None"
   ],
   "id": "a4d1e7da311828df",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:09.822825Z",
     "start_time": "2024-06-07T00:36:09.819191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Operation(object):\n",
    "    \"\"\"\n",
    "    Base class for an \"operation\" in a neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_: ndarray):\n",
    "        \"\"\"\n",
    "        Stores input in the self._input instance variable\n",
    "        Calls the self._output() function.\n",
    "        \"\"\"\n",
    "        self.input_ = input_\n",
    "\n",
    "        self.output = self._output()\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Calls the self._input_grad() function.\n",
    "        Checks that the appropriate shapes match.\n",
    "        \"\"\"\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        The _output method must be defined for each Operation\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        The _input_grad method must be defined for each Operation\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ],
   "id": "6b650473f1545483",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:12.383749Z",
     "start_time": "2024-06-07T00:36:12.380557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ParamOperation(Operation):\n",
    "    \"\"\"\n",
    "    An Operation with parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, param: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        The ParamOperation method\n",
    "        :param param: \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Calls self._input_grad and self._param_grad.\n",
    "        Checks appropriate shapes.\n",
    "        \"\"\"\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        assert_same_shape(self.param, self.param_grad)\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Every subclass of ParamOperation must implement _param_grad.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ],
   "id": "6f80eace56ff7cd9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 神经网络构建块：Layers\n",
    "\n",
    "`Layer` 类将代表神经网络中的一层，它由一个或多个 `Operation` 组成。每个 `Layer` 都有一个 `forward` 方法，该方法接受 `ndarray` 作为输入，并返回 `ndarray` 作为输出。\n",
    "\n",
    "使用 `Operations` 表示神经网络：\n",
    "\n",
    "![layers](./images/03_layers.png)\n",
    "\n",
    "\n",
    "使用 `Layers` 表示神经网络：\n",
    "\n",
    "![layers](./images/03_layers2.png)\n",
    "\n",
    "\n",
    "\n",
    "深度学习模型是一个有着多层隐藏层的神经网络。"
   ],
   "id": "c23770732a10602a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 其他构建块\n",
    "\n",
    "- 输入矩阵与参数矩阵的乘法\n",
    "- 偏置的加法\n",
    "- 激活函数"
   ],
   "id": "f740f217c4405d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:15.477367Z",
     "start_time": "2024-06-07T00:36:15.473922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class WeightMultiply(ParamOperation):\n",
    "    \"\"\"\n",
    "    Weight multiplication operation for a neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, W: ndarray):\n",
    "        \"\"\"\n",
    "        Initialize Operation with self.param = W.\n",
    "        \"\"\"\n",
    "        super().__init__(W)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        Compute output.\n",
    "        \"\"\"\n",
    "        return np.dot(self.input_, self.param)\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Compute input gradient.\n",
    "        \"\"\"\n",
    "        return np.dot(output_grad, np.transpose(self.param, (1, 0)))\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Compute parameter gradient.\n",
    "        \"\"\"\n",
    "        return np.dot(np.transpose(self.input_, (1, 0)), output_grad)"
   ],
   "id": "43e27789d9856fa5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:18.209350Z",
     "start_time": "2024-06-07T00:36:18.205674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BiasAdd(ParamOperation):\n",
    "    \"\"\"\n",
    "    Compute bias addition.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, B: ndarray):\n",
    "        \"\"\"\n",
    "        Initialize Operation with self.param = B.\n",
    "        Check appropriate shape.\n",
    "        \"\"\"\n",
    "        assert B.shape[0] == 1\n",
    "\n",
    "        super().__init__(B)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        Compute output.\n",
    "        \"\"\"\n",
    "        return self.input_ + self.param\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Compute input gradient.\n",
    "        \"\"\"\n",
    "        return np.ones_like(self.input_) * output_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Compute parameter gradient.\n",
    "        \"\"\"\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1])"
   ],
   "id": "b67d1793f097c3c5",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:19.488137Z",
     "start_time": "2024-06-07T00:36:19.485065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sigmoid(Operation):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        Compute output.\n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-1.0 * self.input_))\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Compute input gradient.\n",
    "        \"\"\"\n",
    "        sigmoid_backward = self.output * (1.0 - self.output)\n",
    "        return sigmoid_backward * output_grad"
   ],
   "id": "737975225f1c03e1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:26.329903Z",
     "start_time": "2024-06-07T00:36:26.327758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Linear(Operation):\n",
    "    \"\"\"\n",
    "    \"Identity\" activation function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Pass\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        \"\"\"Pass through\"\"\"\n",
    "        return self.input_\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"Pass through\"\"\"\n",
    "        return output_grad"
   ],
   "id": "b1ca00cc6a0dc33f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:31.196157Z",
     "start_time": "2024-06-07T00:36:31.192262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer(object):\n",
    "    \"\"\"\n",
    "    A \"layer\" of neurons in a neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, neurons: int):\n",
    "        \"\"\"\n",
    "        The number of \"neurons\" roughly corresponds to the \"breadth\" of the layer\n",
    "        \"\"\"\n",
    "        self.neurons = neurons\n",
    "        self.first = True\n",
    "        self.params: List[ndarray] = []\n",
    "        self.param_grads: List[ndarray] = []\n",
    "        self.operations: List[Operation] = []\n",
    "\n",
    "    def _setup_layer(self, num_in: int) -> None:\n",
    "        \"\"\"\n",
    "        The _setup_layer function must be implemented for each layer.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input_: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Passes input forward through a series of operations.\n",
    "        \"\"\"\n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "\n",
    "        self.input_ = input_\n",
    "\n",
    "        for operation in self.operations:\n",
    "            input_ = operation.forward(input_)\n",
    "\n",
    "        self.output = input_\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Passes output_grad backward through a series of operations.\n",
    "        \"\"\"\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "\n",
    "        input_grad = output_grad\n",
    "\n",
    "        self._param_grads()\n",
    "\n",
    "        return input_grad\n",
    "\n",
    "    def _param_grads(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        Extracts the _param_grads from a layer's operations.\n",
    "        \"\"\"\n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "\n",
    "    def _params(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        Extracts the _params from a layer's operations\n",
    "        \"\"\"\n",
    "\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.params.append(operation.param)"
   ],
   "id": "fc72188e56af6e83",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "全连接层有时也称为 `Dense` 层",
   "id": "4e6783ec9a35e1cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:34.694351Z",
     "start_time": "2024-06-07T00:36:34.691188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"\n",
    "    A fully connected layer which inherits from \"Layer\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 neurons: int,\n",
    "                 activation: Operation = Sigmoid()):\n",
    "        \"\"\"\n",
    "        Requires an activation function upon initialization\n",
    "        \"\"\"\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "\n",
    "    def _setup_layer(self, input_: ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Defines the operations of a fully connected layer.\n",
    "        \"\"\"\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        self.params = []\n",
    "\n",
    "        # weights\n",
    "        self.params.append(np.random.randn(input_.shape[1], self.neurons))\n",
    "\n",
    "        # bias\n",
    "        self.params.append(np.random.randn(1, self.neurons))\n",
    "\n",
    "        self.operations = [WeightMultiply(self.params[0]),\n",
    "                           BiasAdd(self.params[1]),\n",
    "                           self.activation]\n",
    "\n",
    "        return None"
   ],
   "id": "9e671da9d57732a1",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 神经网络的构建块：Loss\n",
    "\n",
    "神经网络的“学习”过程：\n",
    "- 矩阵数据 $X$ 作为神经网络的输入，并在前向传播中通过每一个 `Layer`，并最终计算出 `prediction`\n",
    "- 通过 `prediction` 和目标值 `y` 之间的差异，计算出 `loss` 以及 `loss gradient`，也就是 `loss` 相对于网络最后一层中每个元素（即生成 `prediction` 的元素）的偏导数\n",
    "- 在反响传播过程中，将 `loss gradient` 传递回网络的每一层，并计算出参数的梯度，也就是 `loss` 相对于每个参数的偏导数\n",
    "\n",
    "\n",
    "\n",
    "![backpropagation](./images/03_backpropagation.png)"
   ],
   "id": "e888b92ff92fc0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:36.947292Z",
     "start_time": "2024-06-07T00:36:36.943612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Loss(object):\n",
    "    \"\"\"\n",
    "    The \"loss\" of a neural network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Pass\"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, prediction: ndarray, target: ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Computes the actual loss value\n",
    "        \"\"\"\n",
    "        assert_same_shape(prediction, target)\n",
    "\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "\n",
    "        loss_value = self._output()\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def backward(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        Computes gradient of the loss value with respect to the input to the loss function\n",
    "        \"\"\"\n",
    "        self.input_grad = self._input_grad()\n",
    "\n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        \"\"\"\n",
    "        Every subclass of \"Loss\" must implement the _output function.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        Every subclass of \"Loss\" must implement the _input_grad function.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ],
   "id": "4358d1d6f395f526",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:39.393463Z",
     "start_time": "2024-06-07T00:36:39.390061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MeanSquaredError(Loss):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Pass\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        \"\"\"\n",
    "        Computes the per-observation squared error loss\n",
    "        \"\"\"\n",
    "        loss = (\n",
    "                np.sum(np.power(self.prediction - self.target, 2)) /\n",
    "                self.prediction.shape[0]\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        Computes the loss gradient with respect to the input for MSE loss\n",
    "        \"\"\"\n",
    "\n",
    "        return 2.0 * (self.prediction - self.target) / self.prediction.shape[0]"
   ],
   "id": "db860d04c220f2ca",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 神经网络的构建块：Put it together\n",
    "\n",
    "在有了 `Layer`, `Operation`, `Loss` 等操作后，我们如何将这些基础构建块组合在一起，以构建一个完整的神经网络呢？一个 `NeuralNetwork` 需要：\n",
    "- 一组 `Layer`。每个 `Layer` 中有 `forward` 和 `backward` 方法，这些方法接收 `ndarray` 作为输入，并返回 `ndarray` 作为输出\n",
    "- 每个 `Layer` 都有一组 `Operation`，在 `_setup_layer` 函数中设置，并保存在 `operations` 属性中\n",
    "- `Operations` 和 `Layer` 一样，有 `forward` 和 `backward` 方法，这些方法接收 `ndarray` 作为输入，并返回 `ndarray` 作为输出\n",
    "- 在每个 `operation` 中，在 `backward` 函数中的 `output_grad` 的形状，必须与 `Layer` 中的 `output` 的形状相同，对于 `input_grad` 也是如此\n",
    "- 有些 `operation` 有参数，存储在 `param` 属性中\n",
    "- 一个 `NeuralNetwork` 同样也有一个 `Loss`，它的输入参数为 `NeuralNetwork` 最后一个 `operation` 的输出和 `target`\n",
    "\n",
    "\n",
    "我们再描述下 `NeuralNetwork` 整体过程：\n",
    "1. 接收 `X` 和 `y` 作为输入，它们都是 `ndarray`\n",
    "2. 将 `X` 顺序传递给每个 `Layer` 的 `forward` 方法，最终得到 `prediction`\n",
    "3. 使用 `Loss` 计算 `prediction` 和 `y` 之间的 `loss value` 和 `logg gradient`，并传递给 `backward`\n",
    "4. 通过 `backward` 方法，将 `loss gradient` 作为输入，计算每一层 `Layer` 的 `param_gradient`\n",
    "5. 调用每一层 `Layer` 的 `update_param` 方法，改方法将使用神经网络的整体学习率以及新计算的 `param_grads` 来更新参数"
   ],
   "id": "730128a10e17aa2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:42.656843Z",
     "start_time": "2024-06-07T00:36:42.653150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    A Neural Network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers: List[Layer], loss: Loss, seed: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        Neural Network need layers, and a loss\n",
    "        \"\"\"\n",
    "\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.seed = seed\n",
    "        if seed:\n",
    "            for layer in self.layers:\n",
    "                setattr(layer, \"seed\", self.seed)\n",
    "        # self.param_grads: List[ndarray] = []\n",
    "\n",
    "    def forward(self, X_batch: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Passes data forward through a series of layers\n",
    "        \"\"\"\n",
    "\n",
    "        x_out = X_batch\n",
    "        for layer in self.layers:\n",
    "            x_out = layer.forward(x_out)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "    def backward(self, loss_grad: ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Passes data backward through a series of layers\n",
    "        \"\"\"\n",
    "\n",
    "        grad = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def train_batch(self,\n",
    "                    X_batch: ndarray,\n",
    "                    y_batch: ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Passes data forward through the layers.\n",
    "        Computes the loss.\n",
    "        Passes data backward through the layers.\n",
    "        \"\"\"\n",
    "\n",
    "        prediction = self.forward(X_batch)\n",
    "\n",
    "        loss = self.loss.forward(prediction, y_batch)\n",
    "\n",
    "        self.backward(self.loss.backward())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def params(self):\n",
    "        \"\"\"\n",
    "        Gets the parameters for the network.\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "            yield from layer.params\n",
    "\n",
    "    def param_grads(self):\n",
    "        \"\"\"\n",
    "        Gets the gradient of the loss with respect to the parameters for the network.\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "            yield from layer.param_grads"
   ],
   "id": "9d74eae453e28df4",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 神经网络的构建块：Trainer and Optimizer\n",
    "\n",
    "除了上述的 `NeuralNetwork` 外，还需要一个 `Trainer` 和一个 `Optimizer`。`Trainer` 用于训练神经网络，`Optimizer` 用于更新参数。`Trainer` 包括了 `NeuralNetwork` 和 `Optimizer`。"
   ],
   "id": "e2ad53f6926381be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:46.257507Z",
     "start_time": "2024-06-07T00:36:46.254864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Optimizer(object):\n",
    "    \"\"\"\n",
    "    Base class for a neural network optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01):\n",
    "        \"\"\"\n",
    "        Every optimizer must have an initial learning rate.\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self) -> None:\n",
    "        \"\"\"\n",
    "        Every optimizer must implement the \"step\" function.\n",
    "        \"\"\"\n",
    "        pass"
   ],
   "id": "64b9408b74006ca5",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:47.894747Z",
     "start_time": "2024-06-07T00:36:47.892051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SGD(Optimizer):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01) -> None:\n",
    "        \"\"\"Pass\"\"\"\n",
    "        super().__init__(lr)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        For each parameter, adjust in the appropriate direction, with the magnitude of the adjustment \n",
    "        based on the learning rate.\n",
    "        \"\"\"\n",
    "        for (param, param_grad) in zip(self.net.params(),\n",
    "                                       self.net.param_grads()):\n",
    "            param -= self.lr * param_grad"
   ],
   "id": "3ec824cd68a27f9c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:52.320284Z",
     "start_time": "2024-06-07T00:36:52.315487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Trains a neural network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 net: NeuralNetwork,\n",
    "                 optim: Optimizer) -> None:\n",
    "        \"\"\"\n",
    "        Requires a neural network and an optimizer in order for training to occur. \n",
    "        Assign the neural network as an instance variable to the optimizer.\n",
    "        \"\"\"\n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.best_loss = 1e9\n",
    "        setattr(self.optim, 'net', self.net)\n",
    "\n",
    "    def generate_batches(self,\n",
    "                         X: ndarray,\n",
    "                         y: ndarray,\n",
    "                         size: int = 32) -> Tuple[ndarray]:\n",
    "        \"\"\"\n",
    "        Generates batches for training \n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], \\\n",
    "            '''\n",
    "            features and target must have the same number of rows, instead\n",
    "            features has {0} and target has {1}\n",
    "            '''.format(X.shape[0], y.shape[0])\n",
    "\n",
    "        N = X.shape[0]\n",
    "\n",
    "        for ii in range(0, N, size):\n",
    "            X_batch, y_batch = X[ii:ii + size], y[ii:ii + size]\n",
    "\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "    def fit(self, X_train: ndarray, y_train: ndarray,\n",
    "            X_test: ndarray, y_test: ndarray,\n",
    "            epochs: int = 100,\n",
    "            eval_every: int = 10,\n",
    "            batch_size: int = 32,\n",
    "            seed: int = 1,\n",
    "            restart: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Fits the neural network on the training data for a certain number of epochs.\n",
    "        Every \"eval_every\" epochs, it evaluated the neural network on the testing data.\n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        if restart:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "\n",
    "            self.best_loss = 1e9\n",
    "\n",
    "        for e in range(epochs):\n",
    "\n",
    "            if (e + 1) % eval_every == 0:\n",
    "                # for early stopping\n",
    "                last_model = deepcopy(self.net)\n",
    "\n",
    "            X_train, y_train = permute_data(X_train, y_train)\n",
    "\n",
    "            batch_generator = self.generate_batches(X_train, y_train,\n",
    "                                                    batch_size)\n",
    "\n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "                self.net.train_batch(X_batch, y_batch)\n",
    "\n",
    "                self.optim.step()\n",
    "\n",
    "            if (e + 1) % eval_every == 0:\n",
    "\n",
    "                test_preds = self.net.forward(X_test)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "\n",
    "                if loss < self.best_loss:\n",
    "                    print(f\"Validation loss after {e + 1} epochs is {loss:.3f}\")\n",
    "                    self.best_loss = loss\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"\"\"Loss increased after epoch {e + 1}, final loss was {self.best_loss:.3f}, using the model from epoch {e + 1 - eval_every}\"\"\")\n",
    "                    self.net = last_model\n",
    "                    # ensure self.optim is still updating self.net\n",
    "                    setattr(self.optim, 'net', self.net)\n",
    "                    break"
   ],
   "id": "3cc6b667e0a30161",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:53.863996Z",
     "start_time": "2024-06-07T00:36:53.861937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def permute_data(X, y):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    return X[perm], y[perm]"
   ],
   "id": "e24842608b5994ab",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:55.601041Z",
     "start_time": "2024-06-07T00:36:55.597615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluation metrics\n",
    "def mae(y_true: ndarray, y_pred: ndarray):\n",
    "    \"\"\"\n",
    "    Compute mean absolute error for a neural network.\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "\n",
    "def rmse(y_true: ndarray, y_pred: ndarray):\n",
    "    \"\"\"\n",
    "    Compute root mean squared error for a neural network.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean(np.power(y_true - y_pred, 2)))\n",
    "\n",
    "\n",
    "def eval_regression_model(model: NeuralNetwork,\n",
    "                          X_test: ndarray,\n",
    "                          y_test: ndarray):\n",
    "    \"\"\"\n",
    "    Compute mae and rmse for a neural network.\n",
    "    \"\"\"\n",
    "    preds = model.forward(X_test)\n",
    "    preds = preds.reshape(-1, 1)\n",
    "    print(\"Mean absolute error: {:.2f}\".format(mae(preds, y_test)))\n",
    "    print()\n",
    "    print(\"Root mean squared error {:.2f}\".format(rmse(preds, y_test)))"
   ],
   "id": "e3888b76f2703159",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:56.969125Z",
     "start_time": "2024-06-07T00:36:56.965796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lr = NeuralNetwork(\n",
    "    layers=[Dense(neurons=1,\n",
    "                  activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")\n",
    "\n",
    "nn = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13,\n",
    "                  activation=Sigmoid()),\n",
    "            Dense(neurons=1,\n",
    "                  activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")\n",
    "\n",
    "dl = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13,\n",
    "                  activation=Sigmoid()),\n",
    "            Dense(neurons=13,\n",
    "                  activation=Sigmoid()),\n",
    "            Dense(neurons=1,\n",
    "                  activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")"
   ],
   "id": "7ae7c910bb1befd5",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:36:59.841255Z",
     "start_time": "2024-06-07T00:36:59.044633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 准备数据\n",
    "import pandas as pd\n",
    "\n",
    "# New source for Boston housing data per https://scikit-learn.org/1.0/whats_new/v1.0.html#changes-1-0\n",
    "# data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(\"./housing.csv\", sep=\"\\\\s+\", skiprows=22, header=None)\n",
    "\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "features = np.array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS',\n",
    "                     'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'])\n",
    "\n",
    "# Scaling the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)"
   ],
   "id": "ab1779c2fef51dab",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:37:01.247419Z",
     "start_time": "2024-06-07T00:37:01.244658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def to_2d_np(a: ndarray,\n",
    "             type: str = \"col\") -> ndarray:\n",
    "    \"\"\"\n",
    "    Turns a 1D Tensor into 2D\n",
    "    \"\"\"\n",
    "\n",
    "    assert a.ndim == 1, \\\n",
    "        \"Input tensors must be 1 dimensional\"\n",
    "\n",
    "    if type == \"col\":\n",
    "        return a.reshape(-1, 1)\n",
    "    elif type == \"row\":\n",
    "        return a.reshape(1, -1)"
   ],
   "id": "2eae23e573f7068",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:37:03.180349Z",
     "start_time": "2024-06-07T00:37:03.135172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
    "\n",
    "# make target 2d array\n",
    "y_train, y_test = to_2d_np(y_train), to_2d_np(y_test)"
   ],
   "id": "fea5c6a75a8f5d1e",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train the models",
   "id": "bfee56f34bfdc2a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:37:07.280346Z",
     "start_time": "2024-06-07T00:37:07.267851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(lr, SGD(lr=0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "            epochs=50,\n",
    "            eval_every=10,\n",
    "            seed=20190501)\n",
    "print()\n",
    "eval_regression_model(lr, X_test, y_test)"
   ],
   "id": "3d55142d3c67c108",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 18.965\n",
      "Validation loss after 20 epochs is 5.640\n",
      "Validation loss after 30 epochs is 3.228\n",
      "Validation loss after 40 epochs is 2.377\n",
      "Validation loss after 50 epochs is 2.063\n",
      "\n",
      "Mean absolute error: 1.03\n",
      "\n",
      "Root mean squared error 1.44\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:37:18.483370Z",
     "start_time": "2024-06-07T00:37:18.465013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(nn, SGD(lr=0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "            epochs=50,\n",
    "            eval_every=10,\n",
    "            seed=20190501)\n",
    "print()\n",
    "eval_regression_model(nn, X_test, y_test)"
   ],
   "id": "1e898a5fa06c39eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 10.329\n",
      "Validation loss after 20 epochs is 6.254\n",
      "Validation loss after 30 epochs is 4.606\n",
      "Validation loss after 40 epochs is 3.568\n",
      "Validation loss after 50 epochs is 3.004\n",
      "\n",
      "Mean absolute error: 1.39\n",
      "\n",
      "Root mean squared error 1.73\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T00:37:30.236329Z",
     "start_time": "2024-06-07T00:37:30.210827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(dl, SGD(lr=0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "            epochs=50,\n",
    "            eval_every=10,\n",
    "            seed=20190501)\n",
    "print()\n",
    "eval_regression_model(dl, X_test, y_test)"
   ],
   "id": "e58acf81fa4b5d87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 19.001\n",
      "Validation loss after 20 epochs is 8.479\n",
      "Validation loss after 30 epochs is 6.069\n",
      "Validation loss after 40 epochs is 4.749\n",
      "Validation loss after 50 epochs is 4.009\n",
      "\n",
      "Mean absolute error: 1.52\n",
      "\n",
      "Root mean squared error 2.00\n"
     ]
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
